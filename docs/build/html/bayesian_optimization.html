
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Bayesian Optimization &#8212; FPGA_Arch_Exploration  documentation</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/alabaster.css" />
    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Hyperopt Package" href="hyperopt.html" />
    <link rel="prev" title="VIB Interconnect Architecture" href="vib_architecture.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="bayesian-optimization">
<h1>Bayesian Optimization<a class="headerlink" href="#bayesian-optimization" title="Permalink to this headline">¶</a></h1>
<p>Bayesian optimization is a sequential design strategy used for black-box optimization. It leverages information from already explored points to model the unknown function and determines the next most valuable search point, guiding the search process and reducing the number of iterations. It is typically used to optimize expensive evaluation functions. The core idea is to estimate the posterior distribution of the objective function using Bayes’ theorem based on the data, and then select the next sampling parameter combination according to the distribution. In general, Bayesian optimization is suitable for situations where the objective function is unknown and the evaluation cost is high.</p>
<p>Figure 1 shows an experimental diagram of the iterative process of one-dimensional Bayesian optimization. First, the algorithm generates an initial set of solutions, then searches for the next point that is likely to be an extremum based on these points. This point is added to the set, and the process is repeated until the iteration terminates. Finally, the best point from these points is selected as the solution to the problem. The key question here is how to determine the next search point based on the already evaluated points. Bayesian optimization estimates the mean and variance (i.e., the fluctuation range) of the true objective function value using the function values of the already searched points, as shown in Figure 1. The dashed line in the figure represents the estimated objective function values, i.e., the mean objective function value at each point, with the already searched points shown as black solid points. The blue region represents the fluctuation range of the function values at each point, centered around the mean (the dashed line), and proportional to the standard deviation. At the search points, the dashed line passes through the search points, where the variance is smallest, and the variance increases further from the search points. This also aligns with intuition: the function value estimates are less reliable further away from the sampled points. Based on the mean and variance, an acquisition function can be constructed, which estimates the likelihood of each point being an extremum, reflecting the degree to which each point is worth searching. The extremum of this function is the next search point. As shown in Figure 1, the points represented by red triangles are the maxima of the acquisition function, which are the next search points at the end of each iteration.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/Bayes.png"><img alt="_images/Bayes.png" src="_images/Bayes.png" style="height: 300px;" /></a>
</figure>
<p>Figure 1. Schematic diagram of the Bayesian Optimization process.</p>
<p>The core of the Bayesian optimization algorithm consists of two parts: (1) modeling the objective function, which involves calculating the mean and variance of the function values at each point, typically implemented using Gaussian process regression; (2) constructing the acquisition function, which is used to decide at which point to sample during the current iteration.</p>
<p>Figure 2 illustrates the basic flow of the Bayesian optimization algorithm. As shown in the pseudocode, it is based on a Sequential Model-based Global Optimization. It runs experiments one by one, each time attempting better parameters by applying Bayesian inference and updating the surrogate model. Then, it finds the parameters that perform best on the surrogate model, applies these parameters to the true objective function, and updates the surrogate model with the new results. This process is repeated continuously until the maximum number of iterations or time is reached.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="_images/Bayes_code.png"><img alt="_images/Bayes_code.png" src="_images/Bayes_code.png" style="height: 300px;" /></a>
</figure>
<p>Figure 2. The pseudocode of Bayesian optimization.</p>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">FPGA_Arch_Exploration</a></h1>








<h3>Navigation</h3>
<p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="overview.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="vib_architecture.html">VIB Interconnect Architecture</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="hyperopt.html">Hyperopt Package</a></li>
<li class="toctree-l1"><a class="reference internal" href="exploration_platform.html">Exploration Platform Based on Bayesian Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="experimental_results.html">Experimental Results</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
      <li>Previous: <a href="vib_architecture.html" title="previous chapter">VIB Interconnect Architecture</a></li>
      <li>Next: <a href="hyperopt.html" title="next chapter">Hyperopt Package</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2025, Yuanqi Wang.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 4.3.2</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/bayesian_optimization.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>